{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf60037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files found: []\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No CSV found inside the zip. Please set CSV_PATH manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCSV files found:\u001b[39m\u001b[33m\"\u001b[39m, csv_files)\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(csv_files) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo CSV found inside the zip. Please set CSV_PATH manually.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m     CSV_PATH = csv_files[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# auto-load first CSV\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading CSV:\u001b[39m\u001b[33m\"\u001b[39m, CSV_PATH)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No CSV found inside the zip. Please set CSV_PATH manually."
     ]
    }
   ],
   "source": [
    "# Decision Tree classifier: complete end-to-end notebook code\n",
    "# ---------------------------------------------------------\n",
    "# 1) unzip (if needed), load dataset\n",
    "# 2) basic cleaning and EDA\n",
    "# 3) preprocessing (impute, encode)\n",
    "# 4) train/test split, baseline Decision Tree\n",
    "# 5) hyperparameter tuning (GridSearchCV)\n",
    "# 6) evaluate & visualize, save model\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# -----------------------------\n",
    "# Step 0: Install imports (if needed)\n",
    "# -----------------------------\n",
    "# !pip install -q scikit-learn joblib seaborn matplotlib pandas\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Libraries & file paths\n",
    "# -----------------------------\n",
    "import zipfile, os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "\n",
    "# --- Update these paths if needed ---\n",
    "ZIP_PATH = \"C:/Users/Renuka/Desktop/Skillcraft/Dataset/bank+marketing.zip\"      # correct zip file path\n",
    "CSV_PATH = None  # will be set automatically after extraction\n",
    "\n",
    "if ZIP_PATH and os.path.exists(ZIP_PATH):\n",
    "    extract_to = \"dataset_extracted\"\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as z:\n",
    "        z.extractall(extract_to)\n",
    "    # Print all extracted files for debugging\n",
    "    for root, dirs, files in os.walk(extract_to):\n",
    "        for name in files:\n",
    "            print(\"Extracted file:\", os.path.join(root, name))\n",
    "    # find all CSVs inside extracted folder\n",
    "    csv_files = glob.glob(os.path.join(extract_to, '**', '*.csv'), recursive=True)\n",
    "    print(\"CSV files found:\", csv_files)\n",
    "    if len(csv_files) == 0:\n",
    "        raise FileNotFoundError(\"No CSV found inside the zip. Please set CSV_PATH manually.\")\n",
    "    CSV_PATH = csv_files[0]  # auto-load first CSV\n",
    "\n",
    "print(\"Loading CSV:\", CSV_PATH)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Identify target label\n",
    "# -----------------------------\n",
    "# Replace the value below with the exact column name that indicates purchase (label)\n",
    "# Common names: 'Purchase','Purchased','Bought','WillBuy','Buy','Outcome'\n",
    "TARGET_COLUMN = None  # <-- set this to the name of your label column (string)\n",
    "\n",
    "# Try auto-detect if user hasn't set TARGET_COLUMN\n",
    "if TARGET_COLUMN is None:\n",
    "    # look for common names\n",
    "    candidates = ['Purchase','Purchased','Bought','WillBuy','Will_Purchase','Buy','Outcome','Purchased?','Purchase_Flag']\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            TARGET_COLUMN = c\n",
    "            break\n",
    "\n",
    "# fallback: find first column with 2 unique values (binary)\n",
    "if TARGET_COLUMN is None:\n",
    "    for col in df.columns:\n",
    "        if df[col].nunique() == 2:\n",
    "            TARGET_COLUMN = col\n",
    "            break\n",
    "\n",
    "if TARGET_COLUMN is None:\n",
    "    raise ValueError(\"Could not auto-find target column. Please set TARGET_COLUMN to the label column name. Available columns:\\n\" + \", \".join(df.columns))\n",
    "\n",
    "print(\"Using target column:\", TARGET_COLUMN)\n",
    "print(df[TARGET_COLUMN].value_counts(dropna=False))\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Basic cleaning\n",
    "# -----------------------------\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Remove columns with >50% missing data\n",
    "thresh = int(0.5 * len(df))\n",
    "drop_cols = [c for c in df.columns if df[c].isnull().sum() > thresh]\n",
    "# Do not drop the target even if it has missing (we'll handle later)\n",
    "if TARGET_COLUMN in drop_cols:\n",
    "    drop_cols.remove(TARGET_COLUMN)\n",
    "print(\"Dropping columns with >50% missing:\", drop_cols)\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "# If any ID columns (all unique), drop them automatically (likely not useful)\n",
    "id_like = [c for c in df.columns if df[c].nunique() == len(df)]\n",
    "if TARGET_COLUMN in id_like:\n",
    "    id_like.remove(TARGET_COLUMN)\n",
    "print(\"Dropping ID-like columns:\", id_like)\n",
    "df = df.drop(columns=id_like)\n",
    "\n",
    "# Show missing counts\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# If target has missing values, drop those rows\n",
    "df = df[~df[TARGET_COLUMN].isnull()].reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Quick EDA (summary & plots)\n",
    "# -----------------------------\n",
    "print(\"\\n--- Dataset summary ---\")\n",
    "display(df.describe(include='all').transpose())\n",
    "\n",
    "# Plot target distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=TARGET_COLUMN, data=df)\n",
    "plt.title(\"Target distribution\")\n",
    "plt.show()\n",
    "\n",
    "# For numeric columns: histograms\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c != TARGET_COLUMN]  # exclude target if numeric\n",
    "if len(num_cols) > 0:\n",
    "    df[num_cols].hist(bins=20, figsize=(12, 8))\n",
    "    plt.suptitle(\"Histograms for numeric features\")\n",
    "    plt.show()\n",
    "\n",
    "# Correlation heatmap for numeric columns (if >1 numeric)\n",
    "if len(num_cols) > 1:\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(df[num_cols].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "    plt.title(\"Numeric feature correlation\")\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Preprocessing (impute & encode)\n",
    "# -----------------------------\n",
    "# Separate X and y\n",
    "y = df[TARGET_COLUMN].copy()\n",
    "X = df.drop(columns=[TARGET_COLUMN])\n",
    "\n",
    "# Impute missing values:\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
    "        # fill categorical with mode\n",
    "        X[col] = X[col].fillna(X[col].mode()[0])\n",
    "    else:\n",
    "        # fill numerical with median\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Numerical columns:\", num_cols)\n",
    "\n",
    "# Encoding strategy:\n",
    "# - If categorical column has many unique values (>20), use LabelEncoder\n",
    "# - Otherwise use one-hot encoding (get_dummies)\n",
    "high_card = [c for c in cat_cols if X[c].nunique() > 20]\n",
    "onehot = [c for c in cat_cols if c not in high_card]\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# Label encode high-card columns\n",
    "for c in high_card:\n",
    "    X[c] = X[c].astype(str)\n",
    "    X[c] = LabelEncoder().fit_transform(X[c])\n",
    "\n",
    "# One-hot encode the remaining categorical columns\n",
    "if len(onehot) > 0:\n",
    "    X = pd.get_dummies(X, columns=onehot, drop_first=True)\n",
    "\n",
    "print(\"Shape after encoding:\", X.shape)\n",
    "\n",
    "# Encode target into 0/1 if necessary\n",
    "if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "    y_enc = LabelEncoder().fit_transform(y.astype(str))\n",
    "else:\n",
    "    # if numeric but not 0/1, map the two unique values to 0/1\n",
    "    uniq = sorted(y.unique())\n",
    "    if set(uniq) <= {0,1}:\n",
    "        y_enc = y.astype(int)\n",
    "    else:\n",
    "        mapping = {uniq[0]:0, uniq[1]:1}\n",
    "        y_enc = y.map(mapping)\n",
    "\n",
    "y = y_enc\n",
    "\n",
    "print(\"Final X shape:\", X.shape, \"Final y distribution:\\n\", pd.Series(y).value_counts())\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Train/Test split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y if len(np.unique(y))>1 else None\n",
    ")\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 7: Baseline Decision Tree\n",
    "# -----------------------------\n",
    "dt_baseline = DecisionTreeClassifier(random_state=42)\n",
    "dt_baseline.fit(X_train, y_train)\n",
    "y_pred = dt_baseline.predict(X_test)\n",
    "\n",
    "print(\"Baseline accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nBaseline classification report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix plot\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Baseline DT')\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 8: Cross-validation\n",
    "# -----------------------------\n",
    "cv_scores = cross_val_score(dt_baseline, X, y, cv=5, scoring='accuracy')\n",
    "print(\"5-fold CV accuracy:\", np.round(cv_scores, 4))\n",
    "print(\"CV mean accuracy:\", np.round(cv_scores.mean(), 4))\n",
    "\n",
    "# -----------------------------\n",
    "# Step 9: Hyperparameter tuning (GridSearchCV)\n",
    "# -----------------------------\n",
    "param_grid = {\n",
    "    'max_depth': [3,5,7,10,None],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'min_samples_leaf': [1,2,5,10],\n",
    "    'criterion': ['gini','entropy']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                    param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "best_dt = grid.best_estimator_\n",
    "\n",
    "# Evaluate best model\n",
    "y_pred_best = best_dt.predict(X_test)\n",
    "print(\"Best model accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(\"\\nClassification report (best):\\n\", classification_report(y_test, y_pred_best))\n",
    "\n",
    "cm2 = confusion_matrix(y_test, y_pred_best)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm2, annot=True, fmt='d', cmap='Oranges')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Best DT')\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 10: Feature importances & rules\n",
    "# -----------------------------\n",
    "feat_imp = pd.Series(best_dt.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"Top 15 important features:\\n\", feat_imp.head(15))\n",
    "\n",
    "# Plot top features\n",
    "plt.figure(figsize=(8,6))\n",
    "feat_imp.head(15).plot(kind='barh')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Top 15 Feature Importances\")\n",
    "plt.show()\n",
    "\n",
    "# Export textual rules (good for small trees)\n",
    "try:\n",
    "    tree_text = export_text(best_dt, feature_names=list(X.columns))\n",
    "    print(\"\\nDecision tree rules (truncated):\\n\")\n",
    "    print(tree_text[:2000])  # first 2000 chars\n",
    "except Exception as e:\n",
    "    print(\"Could not export textual tree:\", e)\n",
    "\n",
    "# Plot tree (limit depth for readability)\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(best_dt, feature_names=X.columns, class_names=[str(c) for c in best_dt.classes_], filled=True, max_depth=4, fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 11: ROC-AUC (if possible)\n",
    "# -----------------------------\n",
    "if hasattr(best_dt, \"predict_proba\"):\n",
    "    try:\n",
    "        y_proba = best_dt.predict_proba(X_test)[:,1]\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        print(\"ROC AUC:\", auc)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "        plt.plot([0,1],[0,1],'k--')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC Curve - Best Decision Tree\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"ROC/AUC failed:\", e)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 12: Save model\n",
    "# -----------------------------\n",
    "MODEL_PATH = \"decision_tree_model.joblib\"\n",
    "joblib.dump({'model': best_dt, 'features': X.columns.tolist()}, MODEL_PATH)\n",
    "print(\"Saved model to\", MODEL_PATH)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 13: Sample predictions\n",
    "# -----------------------------\n",
    "sample = X_test.copy().iloc[:10].copy()\n",
    "sample['actual'] = y_test[:10].values\n",
    "sample['predicted'] = best_dt.predict(sample.drop(columns=['actual']))\n",
    "display(sample)\n",
    "\n",
    "# -----------------------------\n",
    "# End: Short summary (print)\n",
    "# -----------------------------\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(f\"Dataset rows: {len(df)} | Features after preprocess: {X.shape[1]}\")\n",
    "print(f\"Best DT params: {grid.best_params_}\")\n",
    "print(\"You can load the saved model using joblib.load('decision_tree_model.joblib') and use it for predictions on new data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c3e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Files: ['API_SP.POP.TOTL_DS2_en_csv_v2_763389.csv', 'bank+marketing.zip', 'gender_submission.csv', 'Metadata_Country_API_SP.POP.TOTL_DS2_en_csv_v2_763389.csv', 'Metadata_Indicator_API_SP.POP.TOTL_DS2_en_csv_v2_763389.csv', 'skillcraftdata.zip', 'test.csv', 'titanic.zip', 'train.csv']\n",
      "Using CSV file: API_SP.POP.TOTL_DS2_en_csv_v2_763389.csv\n",
      "\n",
      "🔹 First 5 Rows:\n",
      "                  Country Name Country Code     Indicator Name Indicator Code  \\\n",
      "0                        Aruba          ABW  Population, total    SP.POP.TOTL   \n",
      "1  Africa Eastern and Southern          AFE  Population, total    SP.POP.TOTL   \n",
      "2                  Afghanistan          AFG  Population, total    SP.POP.TOTL   \n",
      "3   Africa Western and Central          AFW  Population, total    SP.POP.TOTL   \n",
      "4                       Angola          AGO  Population, total    SP.POP.TOTL   \n",
      "\n",
      "          1960         1961         1962         1963         1964  \\\n",
      "0      54922.0      55578.0      56320.0      57002.0      57619.0   \n",
      "1  130075728.0  133534923.0  137171659.0  140945536.0  144904094.0   \n",
      "2    9035043.0    9214083.0    9404406.0    9604487.0    9814318.0   \n",
      "3   97630925.0   99706674.0  101854756.0  104089175.0  106388440.0   \n",
      "4    5231654.0    5301583.0    5354310.0    5408320.0    5464187.0   \n",
      "\n",
      "          1965  ...         2016         2017         2018         2019  \\\n",
      "0      58190.0  ...     108727.0     108735.0     108908.0     109203.0   \n",
      "1  149033472.0  ...  623369401.0  640058741.0  657801085.0  675950189.0   \n",
      "2   10036008.0  ...   34700612.0   35688935.0   36743039.0   37856121.0   \n",
      "3  108772632.0  ...  429454743.0  440882906.0  452195915.0  463365429.0   \n",
      "4    5521981.0  ...   29183070.0   30234839.0   31297155.0   32375632.0   \n",
      "\n",
      "          2020         2021         2022         2023         2024  \\\n",
      "0     108587.0     107700.0     107310.0     107359.0     107624.0   \n",
      "1  694446100.0  713090928.0  731821393.0  750503764.0  769294618.0   \n",
      "2   39068979.0   40000412.0   40578842.0   41454761.0   42647492.0   \n",
      "3  474569351.0  485920997.0  497387180.0  509398589.0  521764076.0   \n",
      "4   33451132.0   34532429.0   35635029.0   36749906.0   37885849.0   \n",
      "\n",
      "   Unnamed: 69  \n",
      "0          NaN  \n",
      "1          NaN  \n",
      "2          NaN  \n",
      "3          NaN  \n",
      "4          NaN  \n",
      "\n",
      "[5 rows x 70 columns]\n",
      "\n",
      "🔹 Missing Values:\n",
      "Country Name        0\n",
      "Country Code        0\n",
      "Indicator Name      0\n",
      "Indicator Code      0\n",
      "1960                2\n",
      "                 ... \n",
      "2021                1\n",
      "2022                1\n",
      "2023                1\n",
      "2024                1\n",
      "Unnamed: 69       266\n",
      "Length: 70, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Renuka\\AppData\\Local\\Temp\\ipykernel_10512\\20114656.py:50: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
      "C:\\Users\\Renuka\\AppData\\Local\\Temp\\ipykernel_10512\\20114656.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Missing values handled\n",
      "\n",
      "🔹 Data Types:\n",
      "Country Name       object\n",
      "Country Code       object\n",
      "Indicator Name     object\n",
      "Indicator Code     object\n",
      "1960              float64\n",
      "                   ...   \n",
      "2021              float64\n",
      "2022              float64\n",
      "2023              float64\n",
      "2024              float64\n",
      "Unnamed: 69       float64\n",
      "Length: 70, dtype: object\n",
      "\n",
      "🔹 Summary Statistics:\n",
      "       Country Name Country Code     Indicator Name Indicator Code  \\\n",
      "count           266          266                266            266   \n",
      "unique          266          266                  1              1   \n",
      "top           Aruba          ABW  Population, total    SP.POP.TOTL   \n",
      "freq              1            1                266            266   \n",
      "mean            NaN          NaN                NaN            NaN   \n",
      "std             NaN          NaN                NaN            NaN   \n",
      "min             NaN          NaN                NaN            NaN   \n",
      "25%             NaN          NaN                NaN            NaN   \n",
      "50%             NaN          NaN                NaN            NaN   \n",
      "75%             NaN          NaN                NaN            NaN   \n",
      "max             NaN          NaN                NaN            NaN   \n",
      "\n",
      "                1960          1961          1962          1963          1964  \\\n",
      "count   2.660000e+02  2.660000e+02  2.660000e+02  2.660000e+02  2.660000e+02   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean    1.154023e+08  1.170070e+08  1.191678e+08  1.218381e+08  1.245322e+08   \n",
      "std     3.602853e+08  3.647504e+08  3.713588e+08  3.799269e+08  3.885417e+08   \n",
      "min     2.715000e+03  2.970000e+03  3.264000e+03  3.584000e+03  3.922000e+03   \n",
      "25%     5.197450e+05  5.309200e+05  5.426248e+05  5.549312e+05  5.678438e+05   \n",
      "50%     3.687994e+06  3.794441e+06  3.907216e+06  4.005990e+06  4.084620e+06   \n",
      "75%     2.764304e+07  2.850705e+07  2.941291e+07  3.034163e+07  3.091262e+07   \n",
      "max     3.021513e+09  3.062768e+09  3.117372e+09  3.184063e+09  3.251253e+09   \n",
      "\n",
      "                1965  ...          2016          2017          2018  \\\n",
      "count   2.660000e+02  ...  2.660000e+02  2.660000e+02  2.660000e+02   \n",
      "unique           NaN  ...           NaN           NaN           NaN   \n",
      "top              NaN  ...           NaN           NaN           NaN   \n",
      "freq             NaN  ...           NaN           NaN           NaN   \n",
      "mean    1.272581e+08  ...  3.040350e+08  3.078574e+08  3.116049e+08   \n",
      "std     3.972620e+08  ...  9.429608e+08  9.539567e+08  9.646151e+08   \n",
      "min     4.282000e+03  ...  1.093000e+04  1.086900e+04  1.075100e+04   \n",
      "25%     5.747492e+05  ...  1.791149e+06  1.812532e+06  1.820077e+06   \n",
      "50%     4.166808e+06  ...  1.043844e+07  1.027973e+07  1.037306e+07   \n",
      "75%     3.137105e+07  ...  6.423406e+07  6.447506e+07  6.475391e+07   \n",
      "max     3.318998e+09  ...  7.529067e+09  7.614749e+09  7.697492e+09   \n",
      "\n",
      "                2019          2020          2021          2022          2023  \\\n",
      "count   2.660000e+02  2.660000e+02  2.660000e+02  2.660000e+02  2.660000e+02   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean    3.152801e+08  3.187967e+08  3.219557e+08  3.251253e+08  3.284356e+08   \n",
      "std     9.749599e+08  9.847392e+08  9.934169e+08  1.001838e+09  1.010607e+09   \n",
      "min     1.058100e+04  1.039900e+04  1.019400e+04  9.992000e+03  9.816000e+03   \n",
      "25%     1.810734e+06  1.806644e+06  1.798838e+06  1.810587e+06  1.832749e+06   \n",
      "50%     1.054763e+07  1.069816e+07  1.053749e+07  1.057953e+07  1.075445e+07   \n",
      "75%     6.490552e+07  6.530120e+07  6.594560e+07  6.688096e+07  6.787002e+07   \n",
      "max     7.778304e+09  7.855075e+09  7.920862e+09  7.990400e+09  8.064977e+09   \n",
      "\n",
      "                2024  Unnamed: 69  \n",
      "count   2.660000e+02          0.0  \n",
      "unique           NaN          NaN  \n",
      "top              NaN          NaN  \n",
      "freq             NaN          NaN  \n",
      "mean    3.318869e+08          NaN  \n",
      "std     1.019804e+09          NaN  \n",
      "min     9.646000e+03          NaN  \n",
      "25%     1.809744e+06          NaN  \n",
      "50%     1.087957e+07          NaN  \n",
      "75%     6.854929e+07          NaN  \n",
      "max     8.142056e+09          NaN  \n",
      "\n",
      "[11 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "# 📌 Data Cleaning & EDA Template\n",
    "# Works for any dataset inside a ZIP file\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Import Libraries\n",
    "# ----------------------------\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Unzip & Load Dataset\n",
    "# ----------------------------\n",
    "zip_path = \"C:/Users/Renuka/Desktop/Skillcraft/Dataset/titanic.zip\"  # your zip file\n",
    "extract_path = \"dataset\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# list extracted files to see which CSV is inside\n",
    "extracted_files = os.listdir(extract_path)\n",
    "print(\"Extracted Files:\", extracted_files)\n",
    "\n",
    "# Automatically find the first CSV file in the extracted folder\n",
    "csv_files = [f for f in extracted_files if f.endswith('.csv')]\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(\"No CSV file found in the extracted folder.\")\n",
    "csv_filename = csv_files[0]\n",
    "print(f\"Using CSV file: {csv_filename}\")\n",
    "\n",
    "df = pd.read_csv(os.path.join(extract_path, csv_filename), skiprows=4)\n",
    "\n",
    "print(\"\\n🔹 First 5 Rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Data Cleaning\n",
    "# ----------------------------\n",
    "print(\"\\n🔹 Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Fill missing values\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    else:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "print(\"\\n✅ Missing values handled\")\n",
    "\n",
    "# Check datatypes\n",
    "print(\"\\n🔹 Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Exploratory Data Analysis (EDA)\n",
    "# ----------------------------\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n🔹 Summary Statistics:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Pairplot (only numerical columns)\n",
    "sns.pairplot(df.select_dtypes(include=['float64','int64']))\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram for Age column (if exists)\n",
    "if \"Age\" in df.columns:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.histplot(df[\"Age\"], kde=True, bins=30, color=\"skyblue\")\n",
    "    plt.title(\"Distribution of Age\")\n",
    "    plt.show()\n",
    "\n",
    "# Bar chart for Gender column (if exists)\n",
    "if \"Gender\" in df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x=\"Gender\", data=df, palette=\"Set2\")\n",
    "    plt.title(\"Gender Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "# Group analysis example\n",
    "if \"Gender\" in df.columns and \"Age\" in df.columns:\n",
    "    print(\"\\n🔹 Average Age by Gender:\")\n",
    "    print(df.groupby(\"Gender\")[\"Age\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07b83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
